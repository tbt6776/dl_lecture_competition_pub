{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO+yn11hB1AzBmjJT8CW45K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InDSlM04s_5P","executionInfo":{"status":"ok","timestamp":1721215040637,"user_tz":-540,"elapsed":4199,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}},"outputId":"0a98ca5c-2e32-4556-815c-4718d54f0cc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# import zipfile\n","# import os\n","\n","# # ZIPファイルのパス\n","# src_dir = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/data/train.zip\"\n","\n","# # 展開先のディレクトリ\n","# extract_to = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/data/train\"\n","\n","# # 展開先のディレクトリが存在しない場合は作成\n","# os.makedirs(extract_to, exist_ok=True)\n","\n","# # ZIPファイルを展開\n","# with zipfile.ZipFile(src_dir, 'r') as zip_ref:\n","#     zip_ref.extractall(extract_to)\n","\n","# print(\"ZIPファイルの展開が完了しました。\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvPbxsy3vpUh","executionInfo":{"status":"ok","timestamp":1720585127801,"user_tz":-540,"elapsed":518052,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}},"outputId":"c7eae9eb-8737-4652-9e21-9f51b3fac981"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIPファイルの展開が完了しました。\n"]}]},{"cell_type":"code","source":["# import zipfile\n","# import os\n","\n","# # ZIPファイルのパス\n","# src_dir = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/data/valid.zip\"\n","\n","# # 展開先のディレクトリ\n","# extract_to = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/data/valid\"\n","\n","# # 展開先のディレクトリが存在しない場合は作成\n","# os.makedirs(extract_to, exist_ok=True)\n","\n","# # ZIPファイルを展開\n","# with zipfile.ZipFile(src_dir, 'r') as zip_ref:\n","#     zip_ref.extractall(extract_to)\n","\n","# print(\"ZIPファイルの展開が完了しました。\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNOZCKcq1itJ","executionInfo":{"status":"ok","timestamp":1720586717572,"user_tz":-540,"elapsed":135332,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}},"outputId":"b6856f56-16af-438c-a481-986941825973"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIPファイルの展開が完了しました。\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","from concurrent.futures import ThreadPoolExecutor\n","\n","src_dir = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/data\"\n","dst_dir = \"/content/data\"\n","\n","os.makedirs(dst_dir, exist_ok=True)\n","\n","files_to_copy = []\n","for root, _, files in os.walk(src_dir):\n","    for file in files:\n","        src_file = os.path.join(root, file)\n","        dst_file = os.path.join(dst_dir, os.path.relpath(src_file, src_dir))\n","        files_to_copy.append((src_file, dst_file))\n","\n","def copy_file(src_dst):\n","  src_file, dst_file = src_dst\n","  dst_file_dir = os.path.dirname(dst_file)\n","  os.makedirs(dst_file_dir, exist_ok=True)\n","  try:\n","    shutil.copy2(src_file, dst_file)\n","  except Exception as e:\n","    print(f\"Error copying {src_file} to {dst_file}: {e}\")\n","\n","with ThreadPoolExecutor(max_workers=24) as executor:\n","  executor.map(copy_file, files_to_copy)\n","\n","copied_files = [os.path.join(root, file) for root, _, files in os.walk(dst_dir) for file in files]\n","missing_files = [src for src, dst in files_to_copy if dst not in copied_files]\n","\n","if missing_files:\n","  print(f\"missing files: {missing_files}\")\n","else:\n","  print(\"all files are copied successfully\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kXh7e1ptHjG","executionInfo":{"status":"ok","timestamp":1721215045223,"user_tz":-540,"elapsed":973,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}},"outputId":"3988a5d2-2467-43a6-e32e-ac8a4149373b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["all files are copied successfully\n"]}]},{"cell_type":"code","source":["import re\n","import random\n","import time\n","from statistics import mode\n","\n","from PIL import Image\n","import numpy as np\n","import pandas\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import re\n","from torchvision import transforms\n","import torchvision.models as models\n","\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import DataLoader"],"metadata":{"id":"3NbLEPA6tH81","executionInfo":{"status":"ok","timestamp":1721214739984,"user_tz":-540,"elapsed":8670,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def process_text(text):\n","    text = text.lower()\n","    num_word_to_digit = {\n","        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n","        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n","        'ten': '10'\n","    }\n","    for word, digit in num_word_to_digit.items():\n","        text = text.replace(word, digit)\n","    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n","    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n","    contractions = {\n","        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n","        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n","    }\n","    for contraction, correct in contractions.items():\n","        text = text.replace(contraction, correct)\n","    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n","    text = re.sub(r'\\s+,', ',', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","\n","\n","# 1. データローダーの作成\n","class VQADataset(torch.utils.data.Dataset):\n","    def __init__(self, df_path, image_dir, transform=None, answer=True):\n","        self.transform = transform  # 画像の前処理\n","        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n","        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n","        self.answer = answer\n","\n","        # question / answerの辞書を作成\n","        self.question2idx = {}\n","        self.answer2idx = {}\n","        self.idx2question = {}\n","        self.idx2answer = {}\n","\n","        # 質問文に含まれる単語を辞書に追加\n","        for question in self.df[\"question\"]:\n","            question = process_text(question)\n","            words = question.split(\" \")\n","            for word in words:\n","                if word not in self.question2idx:\n","                    self.question2idx[word] = len(self.question2idx)\n","        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n","\n","        if self.answer:\n","            # 回答に含まれる単語を辞書に追加\n","            for answers in self.df[\"answers\"]:\n","                for answer in answers:\n","                    word = answer[\"answer\"]\n","                    word = process_text(word)\n","                    if word not in self.answer2idx:\n","                        self.answer2idx[word] = len(self.answer2idx)\n","            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n","\n","    def update_dict(self, dataset):\n","        \"\"\"\n","        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n","\n","        Parameters\n","        ----------\n","        dataset : Dataset\n","            訓練データのDataset\n","        \"\"\"\n","        self.question2idx = dataset.question2idx\n","        self.answer2idx = dataset.answer2idx\n","        self.idx2question = dataset.idx2question\n","        self.idx2answer = dataset.idx2answer\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        対応するidxのデータ（画像，質問，回答）を取得．\n","\n","        Parameters\n","        ----------\n","        idx : int\n","            取得するデータのインデックス\n","\n","        Returns\n","        -------\n","        image : torch.Tensor  (C, H, W)\n","            画像データ\n","        question : torch.Tensor  (vocab_size)\n","            質問文をone-hot表現に変換したもの\n","        answers : torch.Tensor  (n_answer)\n","            10人の回答者の回答のid\n","        mode_answer_idx : torch.Tensor  (1)\n","            10人の回答者の回答の中で最頻値の回答のid\n","        \"\"\"\n","        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n","        image = self.transform(image)\n","        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n","        question_words = self.df[\"question\"][idx].split(\" \")\n","        for word in question_words:\n","            try:\n","                question[self.question2idx[word]] = 1  # one-hot表現に変換\n","            except KeyError:\n","                question[-1] = 1  # 未知語\n","\n","        if self.answer:\n","            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n","            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n","\n","            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx)\n","\n","        else:\n","            return image, torch.Tensor(question)\n","\n","    def __len__(self):\n","        return len(self.df)"],"metadata":{"id":"1yn9eJ7jteXg","executionInfo":{"status":"ok","timestamp":1721214798516,"user_tz":-540,"elapsed":1028,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 2. 評価指標の実装\n","# 簡単にするならBCEを利用する\n","def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n","    total_acc = 0.\n","\n","    for pred, answers in zip(batch_pred, batch_answers):\n","        acc = 0.\n","        for i in range(len(answers)):\n","            num_match = 0\n","            for j in range(len(answers)):\n","                if i == j:\n","                    continue\n","                if pred == answers[j]:\n","                    num_match += 1\n","            acc += min(num_match / 3, 1)\n","        total_acc += acc / 10\n","\n","    return total_acc / len(batch_pred)\n","\n","\n","\n","# 3. モデルの実装\n","# ResNetを利用できるようにしておく\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class BottleneckBlock(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n","        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels * self.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels * self.expansion)\n","            )\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module): #dropoutなどを使うのあり，Dropout, DropConnect, 重み減衰 (Weight Decay)\n","    def __init__(self, block, layers):\n","        super().__init__()\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(block, layers[0], 64)\n","        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n","        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n","        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.fc = nn.Linear(512 * block.expansion, 512)\n","\n","    def _make_layer(self, block, blocks, out_channels, stride=1):\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride))\n","        self.in_channels = out_channels * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.in_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","\n","        return x\n","\n","\n","def ResNet50():\n","    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n","\n","\n","\n","class VQAModel_init50(nn.Module):\n","    def __init__(self, vocab_size: int, n_answer: int):\n","        super().__init__()\n","        self.resnet = ResNet50()\n","        self.text_encoder = nn.Linear(vocab_size, 512)\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, n_answer)\n","        )\n","\n","    def forward(self, image, question):\n","        image_feature = self.resnet(image)  # 画像の特徴量\n","        question_feature = self.text_encoder(question)  # テキストの特徴量\n","\n","        x = torch.cat([image_feature, question_feature], dim=1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","\n","class VQAModel_lstm_50(nn.Module): #精度は上がっているが，1エポックで0.02程度しか上がらない\n","    def __init__(self, vocab_size: int, emb_dim: int, hid_dim: int, n_answer: int):\n","        super().__init__()\n","        self.resnet = ResNet50()\n","        self.resnet.fc = nn.Identity()  # Remove the final fully connected layer\n","        self.embedding = nn.Embedding(vocab_size, emb_dim)\n","        self.bilstm = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(2048 + 2 * hid_dim, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, n_answer)\n","        )\n","\n","    def forward(self, image, question):\n","        # Extract image features\n","        image_feature = self.resnet(image)  # 画像の特徴量 (2048次元)\n","\n","        # Embed and encode the question\n","        question = question.long()\n","        embedded = self.embedding(question)  # (batch_size, seq_len, emb_dim)\n","\n","        # Pack the sequence for LSTM\n","        pad_token = 0\n","        question_lengths = (question != pad_token).sum(dim=1).cpu()  # Move to CPU and ensure it's int64\n","        question_lengths = question_lengths.to(torch.int64)\n","        packed = nn.utils.rnn.pack_padded_sequence(embedded, question_lengths, batch_first=True, enforce_sorted=False)\n","        packed_output, (hidden, cell) = self.bilstm(packed)\n","\n","        # Concatenate the final forward and backward hidden states\n","        question_feature = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # (batch_size, 2*hid_dim)\n","\n","        # Concatenate image and question features\n","        x = torch.cat([image_feature, question_feature], dim=1)  # (batch_size, 2048 + 2*hid_dim)\n","        x = self.fc(x)\n","\n","        return x"],"metadata":{"id":"_8j40tW1tf9Z","executionInfo":{"status":"ok","timestamp":1721214799185,"user_tz":-540,"elapsed":5,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 4. 学習の実装\n","def train(model, dataloader, optimizer, criterion, device, scheduler):\n","    model.train()\n","\n","    total_loss = 0\n","    total_acc = 0\n","    simple_acc = 0\n","\n","    start = time.time()\n","    first_batch_time = 0\n","    for item, (image, question, answers, mode_answer) in enumerate(dataloader):\n","        batch_start_time = time.time()\n","\n","        image, question, answer, mode_answer = \\\n","            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n","\n","        pred = model(image, question)\n","        loss = criterion(pred, mode_answer.squeeze())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n","        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n","\n","        if item%10 == 0:\n","            if item == 0:\n","                first_batch_time = time.time() - batch_start_time\n","                all_batch_time = first_batch_time * len(dataloader)\n","                print(f\"1バッチの処理時間: {first_batch_time:.4f} [s]\")\n","                print(f\"1エポックの処理時間: {all_batch_time:.4f} [s]\")\n","\n","            print(item)\n","\n","    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n","\n","\n","def eval(model, dataloader, optimizer, criterion, device):\n","    model.eval()\n","\n","    total_loss = 0\n","    total_acc = 0\n","    simple_acc = 0\n","\n","    start = time.time()\n","    for image, question, answers, mode_answer in dataloader:\n","        image, question, answer, mode_answer = \\\n","            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n","\n","        pred = model(image, question)\n","        loss = criterion(pred, mode_answer.squeeze())\n","\n","        total_loss += loss.item()\n","        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n","        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n","\n","    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"],"metadata":{"id":"nS8yITGdthdk","executionInfo":{"status":"ok","timestamp":1721214799185,"user_tz":-540,"elapsed":4,"user":{"displayName":"田端諒大","userId":"08608406068874992642"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["set_seed(42)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# dataloader / model\n","transform = transforms.Compose([\n","    transforms.RandomResizedCrop(256),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train/train\", transform=transform)\n","test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid/valid\", transform=transform, answer=False)\n","test_dataset.update_dict(train_dataset)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) #50のときは16\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"],"metadata":{"id":"IJPTvkjmaYz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = VQAModel_init50(vocab_size=len(train_dataset.question2idx) + 1, n_answer=len(train_dataset.answer2idx)).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","num_epoch = 30\n","\n","steps_per_epoch = len(train_loader)\n","print(f\"batch_size: {steps_per_epoch}\")\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, epochs=num_epoch, steps_per_epoch=steps_per_epoch)\n","\n","# モデルのチェックポイントのパス\n","# checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/models/QVA_params_model.tar\"\n","checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/models/QVA_params_model_renew.tar\"\n","\n","\n","# チェックポイントのロード\n","checkpoint = torch.load(checkpoint_path)\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","num_epoch = checkpoint[\"epoch\"]\n","\n","num_epoch = 1\n","#bestをチェックポイントからとってこよう\n","best = 0.5785\n","# トレーニングループ\n","for epoch in range(num_epoch):\n","    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device, scheduler)\n","    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n","          f\"train time: {train_time:.2f} [s]\\n\"\n","          f\"train loss: {train_loss:.4f}\\n\"\n","          f\"train acc: {train_acc:.4f}\\n\"\n","          f\"train simple acc: {train_simple_acc:.4f}\")\n","\n","    if train_acc > best:\n","        # チェックポイントを保存\n","        torch.save(\n","            {\n","                \"epoch\": epoch,\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict(),\n","            },\n","            \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/models/QVA_params_model_renew.tar\",\n","        )"],"metadata":{"id":"vOL-uZOdaa6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#評価\n","model.eval()\n","submission = []\n","for image, question in test_loader:\n","    image, question = image.to(device), question.to(device)\n","    with torch.no_grad():  # 評価時には勾配を計算しない\n","        pred = model(image, question)\n","    pred = pred.argmax(1).cpu().item()  # 予測結果を取得\n","    submission.append(pred)\n","\n","submission = [train_dataset.idx2answer[id] for id in submission]\n","submission = np.array(submission)\n","\n","# モデルとサブミッションを保存\n","# model_path = \"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/models/submission.pth\"\n","# torch.save(model.state_dict(), model_path)\n","np.save(\"/content/drive/MyDrive/Colab Notebooks/Final_Assignment/dl_lecture_competition_pub/VQA/models_new_final/submission.npy\", submission)"],"metadata":{"id":"mkEJdFaZ4OxU"},"execution_count":null,"outputs":[]}]}